# analysis.py
# --- Module for Analyzing LLM Responses ---

from textblob import TextBlob
from sentence_transformers import SentenceTransformer, util
from typing import Dict, Any, Optional

# --- Analysis Model Loading ---
# This model will be downloaded on first use.
# It's a good general-purpose model for semantic similarity.
try:
    similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
except Exception as e:
    print(f"Could not load SentenceTransformer model. Semantic similarity will not be available. Error: {e}")
    similarity_model = None


def analyze_response(response: str, ground_truth: Optional[str] = None) -> Dict[str, Any]:
    """
    Performs a series of analyses on the LLM's response.

    Args:
        response: The text generated by the LLM.
        ground_truth: An optional ideal answer to compare against.

    Returns:
        A dictionary containing the analysis results.
    """
    analysis = {}

    # 1. Basic Metrics
    analysis['char_count'] = len(response)
    analysis['word_count'] = len(response.split())

    # 2. Sentiment Analysis
    try:
        blob = TextBlob(response)
        analysis['sentiment'] = {
            'polarity': blob.sentiment.polarity,  # Range [-1.0, 1.0] where -1 is negative, 1 is positive
            'subjectivity': blob.sentiment.subjectivity  # Range [0.0, 1.0] where 0 is objective, 1 is subjective
        }
    except Exception as e:
        analysis['sentiment'] = f"Error during sentiment analysis: {e}"

    # 3. Semantic Similarity (if ground truth is provided)
    if ground_truth and similarity_model:
        try:
            # Encode the responses
            embedding1 = similarity_model.encode(response, convert_to_tensor=True)
            embedding2 = similarity_model.encode(ground_truth, convert_to_tensor=True)
            
            # Compute cosine similarity
            cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)
            similarity_score = cosine_scores.item()
            
            # Normalize to a 0-100 scale for easier interpretation
            analysis['semantic_similarity_score'] = max(0, similarity_score * 100) 
        except Exception as e:
            analysis['semantic_similarity_score'] = f"Error during similarity analysis: {e}"
    elif not similarity_model:
        analysis['semantic_similarity_score'] = "Model not loaded."
    else:
        analysis['semantic_similarity_score'] = "Not applicable (no ground truth provided)."

    return analysis
